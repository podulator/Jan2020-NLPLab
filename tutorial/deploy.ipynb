{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "Using a pre-trained QA with BERT (see alternate notebook \"training.ipydb\" for details), we'll load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview: an example from SQuAD dataset is like below:\n",
    "\n",
    "    (2, \n",
    "    '56be4db0acb8001400a502ee', \n",
    "    'Where did Super Bowl 50 take place?', \n",
    "\n",
    "    'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "    Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "    champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "    Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "    on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "    California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "    anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "    the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "    game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "    feature the Arabic numerals 50.', \n",
    "\n",
    "    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "    in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "\n",
    "    [403, 355, 355])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker in 4 steps\n",
    "\n",
    "1. Preparing the environment \n",
    "2. Grabbing the model, parameters, code etc\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Preparing the environment\n",
    "\n",
    "In this step we create a couple of variables, and call out to a [helper script](files/environment.config) to prepare our environment. \n",
    "\n",
    "This helper script has a guard condition on it, so it only runs once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "model_path = \"{}/model.tar.gz\".format(pwd)\n",
    "\n",
    "# install some requirements specific to this notebook\n",
    "!bash environment.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Grabbing the model, parameters, code etc\n",
    "\n",
    "In this step we grab the pre-trained BERT model, containing parameters, vocabulary file, and all the inference files (code/serve.py, bert/data/qa.py, bert_qa_evaluate.py) from S3 and save to a local file called model.tar.gz.\n",
    "\n",
    "(Note that the serve.py is the \"entry_point\" for Sagemaker to do the inference, and it needs to be under the code/ directory.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"downloading model to : \" + model_path)\n",
    "!aws s3 cp s3://matrow-public-data/bert/model.tar.gz {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's have a look at what's in the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {model_path}\n",
    "!if [ -d ./tmp ]; then rm -rf ./tmp; fi\n",
    "!mkdir -p ./tmp\n",
    "print(\"\\nextracting {}\\n\".format(model_path))\n",
    "!tar -xzf {model_path} --directory ./tmp/\n",
    "!tree ./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting files\n",
    "\n",
    "[serve.py](./tmp/code/serve.py) has two essential functions in it \n",
    "\n",
    "1. ```model_fn``` to load model parameters\n",
    "2. ```transform_fn``` to run model inference given an input\n",
    "\n",
    "Let's have a quick look in the file...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./code/serve.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Building a docker container with everything installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. \n",
    "\n",
    "Here we build a docker container based on the SageMaker MXNet inference container.\n",
    "\n",
    "You can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by having a look at the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we kick of the build process, which is 2 steps.\n",
    "After authenticating to the public repo that our base image comes from, we issue the docker build command.\n",
    "\n",
    "I am assigning the image tag to a variable to re-use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTag = \"my-docker:inference\"\n",
    "\n",
    "!$(aws ecr get-login --no-include-email --region ${REGION} --registry-ids 763104351884)\n",
    "buildCmd = \"docker build --no-cache --build-arg REGION=${{REGION}} -t {} . -f ./Dockerfile\".format(myTag)\n",
    "!{buildCmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using local mode for demonstration purpose. \n",
    "\n",
    "To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR, so the fully managed prediction endpoint can pull it when it is being created.\n",
    "\n",
    "It would look like this ... \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "In this step, we create an MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. \n",
    "\n",
    "As we mentioned previously, to deploy a non-local endpoint for predictions, we would need to push our container to a container registry, like ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "full_path = \"file://{}\".format(model_path)\n",
    "sagemaker_model = MXNetModel(model_data=full_path,\n",
    "                             image=myTag, # local docker image\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.', \n",
    "                             framework_version='1.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance. \n",
    "\n",
    "If you are ready to deploy the model on a remote instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "For this workshop, stay with local, unless you have altered the notebook, and pushed your docker image to ECR already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually submit a inference job. \n",
    "\n",
    "Here we simply grab two datapoints from the SQuAD dataset and pass the examples to our predictor by calling ```predictor.predict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "output = predictor.predict(my_test_examples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrediction output: \\n\\n\")\n",
    "\n",
    "for k in output.keys():\n",
    "    print('{}\\n\\n'.format(output[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. This is non-essential for local testing, but important if you deployed to a remote endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
