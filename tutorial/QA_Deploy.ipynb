{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "After finishing training QA with BERT (the previous notebook \"QA_Training.ipydb\"), let us load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview: an example from SQuAD dataset is like below:\n",
    "\n",
    "    (2, \n",
    "    '56be4db0acb8001400a502ee', \n",
    "    'Where did Super Bowl 50 take place?', \n",
    "\n",
    "    'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "    Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "    champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "    Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "    on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "    California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "    anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "    the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "    game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "    feature the Arabic numerals 50.', \n",
    "\n",
    "    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "    in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "\n",
    "    [403, 355, 355])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd =os.getcwd()\n",
    "print (pwd)\n",
    "!bash environment.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. ```model_fn``` to load model parameters\n",
    "2. ```transform_fn(``` to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile code/serve.py\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "from bert.data.qa import preprocess_dataset, SQuADTransform\n",
    "import bert.bert_qa_evaluate\n",
    "\n",
    "\n",
    "def model_fn(params_path, model_dir = \"\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        params_path: filename of trained BERT model weights, \n",
    "            e.g., params_path = \"bert_qa-7eb11865.params\"\n",
    "        model_dir: The directory where model files are stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        net: a Gluon model,\n",
    "        vocab: the BERT vocabulary,\n",
    "        transform: a SQuADTransform\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = bert_qa_evaluate.BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model: The Gluon model from model_fn()\n",
    "        input_data: The input data, will be a list(tuples) here\n",
    "            Example:\n",
    "            ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "        input_content_type: The request content type, assume json\n",
    "        output_content_type: The (desired) response content type, assume json\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        response payload and output content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "    data = json.loads(input_data)\n",
    "    test_examples_tuples = bert_qa_evaluate._test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)\n",
    "    all_results = bert_qa_evaluate.get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list)\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrievng the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to grab the pre-trained BERT model, containing parameters, vocabulary file, and all the inference files (```code/serve.py```, ```bert/data/qa.py```, ```bert_qa_evaluate.py```) from S3 and save to a ```model.tar.gz``` file. (Note that the ```serve.py``` is the \"entry_point\" for Sagemaker to do the inference, and it needs to be under ```code/``` directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"file://{}/model.tar.gz\".format(pwd)\n",
    "print(\"downloading to : \" +model_path)\n",
    "!aws s3 cp s3://matrow-public-data/bert/model.tar.gz model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "RUN pip list | grep mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h\n",
    "\n",
    "!echo \"region = ${REGION}\"\n",
    "!$(aws ecr get-login --no-include-email --region ${REGION} --registry-ids 763104351884)\n",
    "!docker build --no-cache --build-arg REGION=${REGION} -t my-docker:inference . -f Dockerfile\n",
    "\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If ```serve.py``` does not work, use ```dummy_hosting_module.py``` for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "sagemaker_model = MXNetModel(model_data=model_path,\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.', \n",
    "                             framework_version='1.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc. **The following line will start docker container building.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to submit a inference job. Here we simply grab two datapoints from the SQuAD dataset and pass the examples to our predictor by calling ```predictor.predict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "# mymodel = model_fn(params_path = \"bert_qa-7eb11865.params\")\n",
    "# transform_fn(mymodel, my_test_examples)\n",
    "output = predictor.predict(my_test_examples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrediction output: \\n\\n\")\n",
    "\n",
    "for k in output.keys():\n",
    "    print('{}\\n\\n'.format(output[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
